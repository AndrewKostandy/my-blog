---
title: Exploring Leverage in Multivariable Linear Regression
author: Andrew Kostandy
date: '2018-09-02'
slug: exploring-leverage-in-multivariable-linear-regression
categories: []
tags:
  - r
  - multivariable linear regression
  - leverage
  - mahalanobis distance
header:
  caption: ''
  image: ''
---



<p>In the context of multivariable linear regression, leverage is a distance measure that shows how far an observation is from the center of the multivariate predictor space. Observations with high leverage values would have the <strong>potential</strong> to influence the regression model highly while observations with low leverage values would not. Additionally, leverage can be used to determine if a new observation is close to the predictor space of the observations used to create the model in order to avoid extrapolation.</p>
<p>Once a linear regression model is built, we can get the leverage values of all the observations used in the model from the diagonal values of its hat matrix. However, a key point to remember is that leverage as a measure of distance is not Euclidean, so we shouldn’t expect the distance to be a straight line to the center of the multivariate predictor space. Instead, leverage takes into consideration the predictor correlations and is therefore able to detect multivariate outliers — which are outliers in the full predictor space taken together though they may not be outliers for any predictors individually!</p>
<p>The following example demonstrates the idea. Let’s say we have 17 labeled observations in the predictor space consisting of 2 predictors <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>. The below scatter plot shows the predictor space:</p>
<pre class="r"><code>library(tidyverse)
theme_set(theme_bw())

df &lt;- tibble(x1=c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,11,25),
             x2=c(2,2.5,3.2,4.6,5.3,6.4,7.9,8.4,9.3,10.2,11.5,12.3,13.7,14.1,15.6,4,26),
             y=c(1,3,2,4.3,5.3,7.3,6.1,8.8,9.3,10.9,11.1,12.5,13.2,14.9,15.8,5,25.4),
             obs=1:17)

ggplot(df, aes(x1, x2)) + geom_point() +
  geom_label(aes(label = obs), nudge_y = 2) +
  scale_x_continuous(breaks = seq(0, 30, 5)) +
  scale_y_continuous(breaks = seq(0, 30, 5)) +
  labs(title = &quot;Our Predictor Space&quot;, x = expression(&quot;x&quot;[1]), y = expression(&quot;x&quot;[2]))</code></pre>
<p><img src="/post/2018-09-02-exploring-leverage-in-linear-regression_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>Notice that observation 16 above is not an outlier in the <span class="math inline">\(x_1\)</span> space by itself or in the <span class="math inline">\(x_2\)</span> space by itself. However, it is a multivariate outlier in the predictor space of <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> together since it’s out of the “oval” region that those 2 predictors have together which is represented below with a 95% confidence level ellipse for demonstrative purposes:</p>
<pre class="r"><code>ggplot(df, aes(x1, x2)) + geom_point() +
  geom_label(aes(label = obs), nudge_y = 3) +
  scale_x_continuous(breaks = seq(-5, 30, 5)) +
  scale_y_continuous(breaks = seq(-5, 30, 5)) +
  stat_ellipse(level = 0.95) +
  labs(title = &quot;Our Predictor Space&quot;, x = expression(&quot;x&quot;[1]), y = expression(&quot;x&quot;[2]))</code></pre>
<p><img src="/post/2018-09-02-exploring-leverage-in-linear-regression_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>Observation 17 above on the other hand is an outlier in both the <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> predictor spaces separately and also an outlier to the region of both predictors together since it’s far away from the center of the multivariate predictor space despite being true to the correlation relationship between the predictors.</p>
<p>Now let’s fit a linear model between a third variable <span class="math inline">\(y\)</span> as the outcome and our variables <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> as predictors. The model equation will therefore be: <span class="math display">\[\hat{y} = b_0 + b_1x_1 + b_2x_2\]</span></p>
<p>Below is a bar plot showing the leverage of the observations in the model:</p>
<pre class="r"><code>model &lt;- lm(y ~ x1 + x2, data = df)

lev_obs &lt;- tibble(leverage = hatvalues(model), obs = 1:17)

ggplot(lev_obs, aes(obs, leverage)) + geom_col() +
  scale_x_continuous(breaks = seq(1, 17)) +
  labs(title = &quot;Observation Leverage Values&quot;, x = &quot;Observation&quot;, y = &quot;Leverage&quot;)</code></pre>
<p><img src="/post/2018-09-02-exploring-leverage-in-linear-regression_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Notice how the leverage metric (from the diagonal of the hat matrix) was able to successfully detect that both observations 16 and 17 were far from center of the multivariate oval region of the predictors.</p>
<p>Observations that are not outliers for the predictors individually but are outliers for the predictors when taken together (the oval) will still be detected by the leverage metric. Therefore, when we have a new observation that we want to predict (<span class="math inline">\(h_0\)</span>), we need to make sure that its leverage is less than the maximum leverage value (<span class="math inline">\(h_{max}\)</span>) used in the model to ensure it’s within the multivariate predictor region covered by the model and that no extrapolation will take place.</p>
<p>Tangentially, the Mahalanobis distance is a similar distance measure that we can use for the same purpose as it can also measure distances from the center of multivariate predictor regions where predictor correlations are also taken into account. It is also very easy to calculate for new observations and compare to the Mahalanobis distances of the observations used in a model.</p>
<p>Below are the Mahalanobis distances of our example above:</p>
<pre class="r"><code>df_temp &lt;- select(df, x1, x2)

means &lt;- colMeans(df_temp)
cov_matrix &lt;- cov(df_temp)

mahalanbis_distances &lt;- mahalanobis(df_temp, means, cov_matrix)

mh &lt;- tibble(obs = 1:17, maha = mahalanbis_distances)

ggplot(mh, aes(obs, maha)) + geom_col() +
  scale_x_continuous(breaks = seq(1, 17)) +
  labs(title = &quot;Observation Mahalanobis Distance Values&quot;,
       x = &quot;Observation&quot;, y = &quot;Mahalanobis Distance&quot;)</code></pre>
<p><img src="/post/2018-09-02-exploring-leverage-in-linear-regression_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Notice that while the Mahalanobis distance values are different from the leverage values, the shape of the plot is identical which indicates that we are measuring the same phenomenon. Therefore, we should be able to compare either metric for a new observation that we want to predict to the maximum value of that metric in our data. This allows us to determine if we are extrapolating or not. However, this can be misleading if our model is not free from outliers such as in our example above. In such cases, we may better assess it by determining how many multiples of the average metric value (leverage or Mahalanobis distance) this new observation is. If observations 16 and 17 had been excluded before building the model, then comparing it to the maximum metric value would be sensible.</p>
<p>In fact, the leverage and the Mahalanobis distance are related through the equation: <span class="math display">\[ \text{Mahalanobis}_i=(N-1)(H_{ii}-\frac{1}{N})\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(N\)</span> is the total number of observations used in the model</li>
<li><span class="math inline">\(H_{ii}\)</span> is the diagonal value of the hat matrix of the model providing the leverage value of observation <span class="math inline">\(i\)</span></li>
</ul>
